{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2560015,"sourceType":"datasetVersion","datasetId":1549969},{"sourceId":8055206,"sourceType":"datasetVersion","datasetId":4750853},{"sourceId":8115247,"sourceType":"datasetVersion","datasetId":4794424},{"sourceId":171944458,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Установим необходимые библиотеки","metadata":{}},{"cell_type":"code","source":"!pip install peft\n!pip install evaluate\n!pip install augmentex","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:18:27.015711Z","iopub.execute_input":"2024-04-28T11:18:27.016127Z","iopub.status.idle":"2024-04-28T11:19:08.451683Z","shell.execute_reply.started":"2024-04-28T11:18:27.016093Z","shell.execute_reply":"2024-04-28T11:19:08.450560Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.38.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.28.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.21.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.10.0\nCollecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.21.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\nCollecting augmentex\n  Downloading augmentex-1.2.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from augmentex) (1.26.4)\nRequirement already satisfied: python-Levenshtein>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from augmentex) (0.25.0)\nRequirement already satisfied: Levenshtein==0.25.0 in /opt/conda/lib/python3.10/site-packages (from python-Levenshtein>=0.22.0->augmentex) (0.25.0)\nRequirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from Levenshtein==0.25.0->python-Levenshtein>=0.22.0->augmentex) (3.6.2)\nDownloading augmentex-1.2.1-py3-none-any.whl (22.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: augmentex\nSuccessfully installed augmentex-1.2.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport wandb\n\nfrom pathlib import Path\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizerFast, BertConfig, BertForTokenClassification, AutoModel, AutoTokenizer\nfrom datasets import Dataset\nfrom transformers import Trainer, TrainingArguments, DataCollatorWithPadding\nfrom safetensors import safe_open\nfrom safetensors.torch import load_model\nfrom evaluate import evaluator\nfrom peft import LoraConfig, get_peft_model\nfrom sklearn.preprocessing import LabelEncoder\nfrom augmentex import WordAug, CharAug\n\nrusentne = Path('/kaggle/input/rusentne')\nintents = Path('/kaggle/input/qa-intents-dataset-university-domain')","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:19:08.453728Z","iopub.execute_input":"2024-04-28T11:19:08.454042Z","iopub.status.idle":"2024-04-28T11:19:29.917948Z","shell.execute_reply.started":"2024-04-28T11:19:08.454014Z","shell.execute_reply":"2024-04-28T11:19:29.916992Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-28 11:19:20.420643: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-28 11:19:20.420765: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-28 11:19:20.546934: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Будем работать с данными с [соревнования](https://github.com/dialogue-evaluation/RuSentNE-evaluation) на Kaggle по анализу тональности к именованным сущностям в новостных текстах.","metadata":{"id":"c9e374ad"}},{"cell_type":"markdown","source":"В данных есть заранее размеченные сущности в рамках отдельного предложения и метка тональности (нейтральное, положительное, отрицательное). В качестве метрики качества будем использовать F1-macro с усреднением по положительным и отрицательным меткам.","metadata":{}},{"cell_type":"markdown","source":"В качестве модели будем исользовать RuBERT с возможно, некоторыми модификациями.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(rusentne / 'train_data.csv', sep='\\t')\nvalid_df = pd.read_csv(rusentne / 'validation_data_labeled.csv', sep='\\t')\ntest_df = pd.read_csv(rusentne / 'final_data.csv', sep='\\t')","metadata":{"id":"341a2656","execution":{"iopub.status.busy":"2024-04-14T10:07:35.468218Z","iopub.execute_input":"2024-04-14T10:07:35.468531Z","iopub.status.idle":"2024-04-14T10:07:35.638082Z","shell.execute_reply.started":"2024-04-14T10:07:35.468502Z","shell.execute_reply":"2024-04-14T10:07:35.637064Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(examples[\"tokens\"],\n                                 is_split_into_words=True,\n                                 return_tensors='pt')\n    labels = []\n\n    for i, label in enumerate(examples[f\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n        label_ids = []\n        for word_idx in word_ids:  # Set the special tokens to -100.\n            if word_idx is None:\n                label_ids.append(-100)\n            else:  # Only label the first token of a given word.\n                label_ids.append(label[word_idx])\n        labels.append(label_ids)\n    tokenized_inputs[\"start\"] = [(torch.argwhere(torch.tensor(labels) != -100)).min().item()]\n    tokenized_inputs[\"end\"] = [(torch.argwhere(torch.tensor(labels) != -100)).max().item()]\n    tokenized_inputs[\"labels\"] = examples[\"labels\"]\n    \n    ret = {}\n    for k, v in tokenized_inputs.items():\n        ret[k] = v[0]\n        \n    return ret\n    \n    \ndef preprocess_datasets(el):\n    sentence = el['sentence']\n    start = el['entity_pos_start_rel']\n    end = el['entity_pos_end_rel']\n    sentence = [sentence[:start]] + [sentence[start:end]] + [sentence[end:]]\n    label = el['label']\n    if label == -1:\n        label = 0\n    elif label == 0:\n        label = 2\n    ner_tags = [-100, label, -100]\n    d = {'tokens' : [sentence], 'ner_tags' : [ner_tags], 'labels' : [label]}\n    \n    return tokenize_and_align_labels(d)\n\ntokenizer = AutoTokenizer.from_pretrained('ai-forever/ruBert-base')\ncolumns_remove = [\"label\", \"sentence\", 'entity', 'entity_tag', 'entity_pos_start_rel', 'entity_pos_end_rel', 'token_type_ids']\ntrain_dataset = Dataset.from_dict(train_df).map(preprocess_datasets).remove_columns(columns_remove)\nval_dataset = Dataset.from_dict(valid_df).map(preprocess_datasets).remove_columns(columns_remove)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:08:19.546882Z","iopub.execute_input":"2024-04-14T10:08:19.547846Z","iopub.status.idle":"2024-04-14T10:08:28.363377Z","shell.execute_reply.started":"2024-04-14T10:08:19.547810Z","shell.execute_reply":"2024-04-14T10:08:28.362421Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/590 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3b12b3811934ec88e2a56fa5af251fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a85c4293fc314edd90c270e4db4e3f0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6637 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ae67292e164e059c554e468fb24eb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2845 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"120f63cc4fe74a1e99ef9727e4e22f41"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:08:28.365061Z","iopub.execute_input":"2024-04-14T10:08:28.365465Z","iopub.status.idle":"2024-04-14T10:08:28.374550Z","shell.execute_reply.started":"2024-04-14T10:08:28.365431Z","shell.execute_reply":"2024-04-14T10:08:28.373678Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101,\n  47351,\n  1622,\n  381,\n  151,\n  94225,\n  687,\n  150,\n  27165,\n  58329,\n  380,\n  160,\n  263,\n  27082,\n  160,\n  76650,\n  66365,\n  478,\n  158,\n  98780,\n  474,\n  33855,\n  121,\n  5077,\n  789,\n  100579,\n  2568,\n  7600,\n  55367,\n  45670,\n  843,\n  17693,\n  9608,\n  10980,\n  121,\n  750,\n  1778,\n  3280,\n  9338,\n  126,\n  102],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1],\n 'start': 0,\n 'end': 31,\n 'labels': 2}"},"metadata":{}}]},{"cell_type":"code","source":"class RuBertClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained('ai-forever/ruBert-base')\n        for param in self.bert.parameters():\n            param.requires_grad = False\n        self.classifier = nn.Sequential(\n                nn.Linear(768, 768),\n                nn.ReLU(),\n                nn.Linear(768, 3)\n        )\n    \n    def forward(self, input_ids, attention_mask, start, end, labels):\n#         input_ids = x['input_ids']\n#         attention_mask = x['attention_mask']\n#         start = x['start']\n#         end = x['end']\n        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n        \n        ranges = [torch.arange(start[i], end[i]).int().tolist() for i in range(len(input_ids))]\n        \n        clsvecs = torch.stack([out[i, ranges[i], :].mean(dim=0) for i in range(len(input_ids))])\n        out = self.classifier(clsvecs)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:28:53.463351Z","iopub.execute_input":"2024-04-14T10:28:53.464045Z","iopub.status.idle":"2024-04-14T10:28:53.475464Z","shell.execute_reply.started":"2024-04-14T10:28:53.464008Z","shell.execute_reply":"2024-04-14T10:28:53.474401Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"roflan = RuBertClassifier().to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:30:52.989314Z","iopub.execute_input":"2024-04-14T10:30:52.989958Z","iopub.status.idle":"2024-04-14T10:30:54.249168Z","shell.execute_reply.started":"2024-04-14T10:30:52.989927Z","shell.execute_reply":"2024-04-14T10:30:54.248098Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Модель обучалась ранее, сохранили чекпоинт","metadata":{}},{"cell_type":"code","source":"load_model(roflan, '/kaggle/input/notebookfdc54ea563/checkpoint-10790/model.safetensors')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T20:28:56.464640Z","iopub.execute_input":"2024-04-13T20:28:56.464930Z","iopub.status.idle":"2024-04-13T20:29:03.085882Z","shell.execute_reply.started":"2024-04-13T20:28:56.464905Z","shell.execute_reply":"2024-04-13T20:29:03.084778Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(set(), [])"},"metadata":{}}]},{"cell_type":"markdown","source":"Добавим модификации в виде LoRA адаптеров","metadata":{}},{"cell_type":"code","source":"config = LoraConfig(\n    r=32,\n    lora_alpha=16,\n    target_modules=[\"query\", \"value\"],\n    lora_dropout=0.5,\n    bias=\"none\",\n    modules_to_save=[\"classifier\"],\n)\nroflan = get_peft_model(roflan, config)\nroflan.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T09:12:15.822793Z","iopub.execute_input":"2024-04-14T09:12:15.823625Z","iopub.status.idle":"2024-04-14T09:12:15.896366Z","shell.execute_reply.started":"2024-04-14T09:12:15.823594Z","shell.execute_reply":"2024-04-14T09:12:15.895486Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"trainable params: 1,772,547 || all params: 180,672,774 || trainable%: 0.9810814107498012\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_loss(model, inputs, return_outputs=False):\n    labels = inputs.get(\"labels\")\n    logits = model(**inputs)\n    loss = nn.CrossEntropyLoss()(logits.squeeze(), labels.squeeze())\n    if return_outputs:\n        return loss, {\"logits\" : logits}\n    return loss\n\ndef compute_f1_score(eval_preds):\n    preds, targets = eval_preds\n    preds = torch.tensor(preds)\n    preds = preds.argmax(dim=-1)\n    targets = torch.tensor(targets)\n\n    idx = torch.argwhere(targets < 2).squeeze()\n    targets = targets[idx]\n    preds = preds[idx]\n    \n    tp = ((preds == targets) * (targets == 1)).sum(axis=-1)\n    fn = ((preds != targets) * (targets == 1)).sum(axis=-1)\n    fp = ((preds != targets) * (targets == 0)).sum(axis=-1)\n\n    eps = 1e-5\n    precision = tp / (tp + fp + eps)\n    recall = tp / (tp + fn + eps)\n\n    f1 = 2 * precision * recall / (precision + recall + eps)\n#     if log:\n#         wandb.log({\"f1_score\": f1.mean()})\n\n    return {'f1_score' : f1.mean()}","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:12:52.879818Z","iopub.execute_input":"2024-04-14T10:12:52.880163Z","iopub.status.idle":"2024-04-14T10:12:52.889563Z","shell.execute_reply.started":"2024-04-14T10:12:52.880136Z","shell.execute_reply":"2024-04-14T10:12:52.888627Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    '/kaggle/working',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit = 1,\n    learning_rate=3e-4,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=30,\n    weight_decay=0.01,\n    metric_for_best_model=\"f1_score\",\n    load_best_model_at_end=True,\n    remove_unused_columns=False,\n    label_names=['labels'],\n    report_to=\"wandb\"\n)\n\ntrainer = Trainer(\n        roflan,\n        training_args,                               \n        train_dataset=train_dataset,        \n        eval_dataset=val_dataset,        \n        data_collator=DataCollatorWithPadding(tokenizer),\n        compute_metrics=compute_f1_score,\n)\ntrainer.compute_loss = compute_loss","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:31:15.814561Z","iopub.execute_input":"2024-04-14T10:31:15.815364Z","iopub.status.idle":"2024-04-14T10:31:15.838335Z","shell.execute_reply.started":"2024-04-14T10:31:15.815334Z","shell.execute_reply":"2024-04-14T10:31:15.837387Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"Обучим модель с адаптерами.","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T09:12:25.379069Z","iopub.execute_input":"2024-04-14T09:12:25.379777Z","iopub.status.idle":"2024-04-14T09:40:29.751456Z","shell.execute_reply.started":"2024-04-14T09:12:25.379743Z","shell.execute_reply":"2024-04-14T09:40:29.750446Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxdoni4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240414_091225-y3madecv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/xdoni4/huggingface/runs/y3madecv' target=\"_blank\">golden-snowflake-21</a></strong> to <a href='https://wandb.ai/xdoni4/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/xdoni4/huggingface' target=\"_blank\">https://wandb.ai/xdoni4/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/xdoni4/huggingface/runs/y3madecv' target=\"_blank\">https://wandb.ai/xdoni4/huggingface/runs/y3madecv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24900' max='24900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24900/24900 27:32, Epoch 30/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.676800</td>\n      <td>0.606728</td>\n      <td>0.235126</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.556300</td>\n      <td>0.581552</td>\n      <td>0.280448</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.501400</td>\n      <td>0.573611</td>\n      <td>0.376530</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.436100</td>\n      <td>0.638662</td>\n      <td>0.296058</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.385600</td>\n      <td>0.666257</td>\n      <td>0.428566</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.333800</td>\n      <td>0.706554</td>\n      <td>0.496058</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.293200</td>\n      <td>0.795841</td>\n      <td>0.459765</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.268100</td>\n      <td>0.867640</td>\n      <td>0.416064</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.229400</td>\n      <td>0.950992</td>\n      <td>0.379209</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.210500</td>\n      <td>0.983571</td>\n      <td>0.441049</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.204900</td>\n      <td>0.970114</td>\n      <td>0.499305</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.182000</td>\n      <td>1.102733</td>\n      <td>0.447509</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.149200</td>\n      <td>1.144034</td>\n      <td>0.529243</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.149800</td>\n      <td>1.198884</td>\n      <td>0.436514</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.135200</td>\n      <td>1.333697</td>\n      <td>0.481370</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.107300</td>\n      <td>1.444775</td>\n      <td>0.408774</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.125600</td>\n      <td>1.463785</td>\n      <td>0.499256</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.110900</td>\n      <td>1.530768</td>\n      <td>0.461309</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.108000</td>\n      <td>1.584718</td>\n      <td>0.457926</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.099400</td>\n      <td>1.542722</td>\n      <td>0.489966</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.085500</td>\n      <td>1.630197</td>\n      <td>0.513547</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.072200</td>\n      <td>1.687120</td>\n      <td>0.482950</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.073300</td>\n      <td>1.780223</td>\n      <td>0.454671</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.074800</td>\n      <td>1.776007</td>\n      <td>0.465313</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.052600</td>\n      <td>1.852045</td>\n      <td>0.449148</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.063400</td>\n      <td>1.846298</td>\n      <td>0.474193</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.059700</td>\n      <td>1.810600</td>\n      <td>0.494313</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.052300</td>\n      <td>1.880027</td>\n      <td>0.454156</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.048600</td>\n      <td>1.876209</td>\n      <td>0.483543</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.042700</td>\n      <td>1.871455</td>\n      <td>0.495045</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Checkpoint destination directory /kaggle/working/checkpoint-4150 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=24900, training_loss=0.19686709017159948, metrics={'train_runtime': 1683.913, 'train_samples_per_second': 118.242, 'train_steps_per_second': 14.787, 'total_flos': 0.0, 'train_loss': 0.19686709017159948, 'epoch': 30.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"Наилучший F1 на валидации получился 0.51","metadata":{}},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T09:12:01.252623Z","iopub.execute_input":"2024-04-14T09:12:01.253003Z","iopub.status.idle":"2024-04-14T09:12:03.417189Z","shell.execute_reply.started":"2024-04-14T09:12:01.252972Z","shell.execute_reply":"2024-04-14T09:12:03.416471Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.053 MB of 0.053 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1_score</td><td>▁▂▅▃█</td></tr><tr><td>eval/loss</td><td>▃▁▁▆█</td></tr><tr><td>eval/runtime</td><td>▁▂▃▃█</td></tr><tr><td>eval/samples_per_second</td><td>█▇▆▆▁</td></tr><tr><td>eval/steps_per_second</td><td>█▇▆▆▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▃▄▄▄▅▆▆▇▇█</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>train/grad_norm</td><td>▅▄▂▅██▂▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▇▆▅▅▃▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1_score</td><td>0.43452</td></tr><tr><td>eval/loss</td><td>0.69885</td></tr><tr><td>eval/runtime</td><td>8.542</td></tr><tr><td>eval/samples_per_second</td><td>333.062</td></tr><tr><td>eval/steps_per_second</td><td>41.677</td></tr><tr><td>train/epoch</td><td>5.42</td></tr><tr><td>train/global_step</td><td>4500</td></tr><tr><td>train/grad_norm</td><td>1.15521</td></tr><tr><td>train/learning_rate</td><td>0.00025</td></tr><tr><td>train/loss</td><td>0.2957</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">balmy-glade-20</strong> at: <a href='https://wandb.ai/xdoni4/huggingface/runs/x2eul0sd' target=\"_blank\">https://wandb.ai/xdoni4/huggingface/runs/x2eul0sd</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240414_090602-x2eul0sd/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"roflan = RuBertClassifier().to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:29:16.547091Z","iopub.execute_input":"2024-04-14T10:29:16.547478Z","iopub.status.idle":"2024-04-14T10:29:17.750540Z","shell.execute_reply.started":"2024-04-14T10:29:16.547448Z","shell.execute_reply":"2024-04-14T10:29:17.749514Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"До этого файнтюнили бейзлайн RuBERT. Дообучали классификатор на выходах модели. Без заморозки весов модель плохо обучается, так как датасет двовольно небольшой. Поэтому в дальнейшем бейзлайн замораживался","metadata":{}},{"cell_type":"code","source":"load_model(roflan, '/kaggle/input/rusentne-rubert-ft/checkpoint-10790/model.safetensors')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:29:19.393401Z","iopub.execute_input":"2024-04-14T10:29:19.394245Z","iopub.status.idle":"2024-04-14T10:29:19.670583Z","shell.execute_reply.started":"2024-04-14T10:29:19.394209Z","shell.execute_reply":"2024-04-14T10:29:19.669687Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(set(), [])"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:29:41.675522Z","iopub.execute_input":"2024-04-14T10:29:41.676332Z","iopub.status.idle":"2024-04-14T10:29:49.635359Z","shell.execute_reply.started":"2024-04-14T10:29:41.676296Z","shell.execute_reply":"2024-04-14T10:29:49.634122Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='356' max='356' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [356/356 00:07]\n    </div>\n    "},"metadata":{}},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.7416484355926514,\n 'eval_f1_score': 0.4456183612346649,\n 'eval_runtime': 7.9429,\n 'eval_samples_per_second': 358.182,\n 'eval_steps_per_second': 44.82}"},"metadata":{}}]},{"cell_type":"markdown","source":"Результат получился хуже чем для модели с адаптерами. Вообще обучение было очень нестабильным во всех случаях. Модель сильно переобучалась в плане лосса: на протяжении всего обучения лосс на трейне падал, а на валидации рос. При этом это не влияло сильно на метрику на валидации (она просто флуктуировала в районе 30-50%), то есть, несмотря на переобучение, модель выучивала необходимые зависимости из трейна. Если решить проблемы с переобучением, то возможно получились бы результаты еще лучше. Но датасет увеличить не получится, а как эффективно аугментировать данные для задачи непонятно, возможно с майнингом данных с помощью обращения к LLM.","metadata":{}},{"cell_type":"markdown","source":"Теперь будем работать над intent classification","metadata":{}},{"cell_type":"markdown","source":"Будем работать с [данными](https://www.kaggle.com/datasets/constantinwerner/qa-intents-dataset-university-domain), которые собраны Новосибирским Государственным Университетом для своего QA-чатбота. Выбор в большой степени обусловлен именно \"студенческим доменом\". Описание гласит, что датасет содержит 142 интента (класса) и порядка 50-220 фраз на русском языке для каждого.","metadata":{"id":"36743704"}},{"cell_type":"code","source":"train_df = pd.read_csv(intents / 'dataset_train.tsv', sep='\\t', names=['phrase', 'intent'])\nvalid_df = pd.read_csv(intents / 'dataset_test.tsv', sep='\\t', names=['phrase', 'intent'])","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:19:29.919196Z","iopub.execute_input":"2024-04-28T11:19:29.919847Z","iopub.status.idle":"2024-04-28T11:19:29.986504Z","shell.execute_reply.started":"2024-04-28T11:19:29.919820Z","shell.execute_reply":"2024-04-28T11:19:29.985417Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Размеры датасетов","metadata":{}},{"cell_type":"code","source":"len(train_df), len(valid_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:19:29.989295Z","iopub.execute_input":"2024-04-28T11:19:29.989591Z","iopub.status.idle":"2024-04-28T11:19:29.996493Z","shell.execute_reply.started":"2024-04-28T11:19:29.989567Z","shell.execute_reply":"2024-04-28T11:19:29.995564Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(13230, 883)"},"metadata":{}}]},{"cell_type":"markdown","source":"Примеров в среднем на класс","metadata":{}},{"cell_type":"code","source":"train_df.groupby('intent').count()[\"phrase\"].mean()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:19:29.997665Z","iopub.execute_input":"2024-04-28T11:19:29.997963Z","iopub.status.idle":"2024-04-28T11:19:30.022842Z","shell.execute_reply.started":"2024-04-28T11:19:29.997922Z","shell.execute_reply":"2024-04-28T11:19:30.021854Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"93.16901408450704"},"metadata":{}}]},{"cell_type":"code","source":"train_df['phrase'].str.split().apply(len).mean()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:19:30.024154Z","iopub.execute_input":"2024-04-28T11:19:30.024826Z","iopub.status.idle":"2024-04-28T11:19:30.055758Z","shell.execute_reply.started":"2024-04-28T11:19:30.024792Z","shell.execute_reply":"2024-04-28T11:19:30.054777Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"4.046636432350718"},"metadata":{}}]},{"cell_type":"markdown","source":"Тексты короткие, в основном словосочетания типа \"мне нужно X\", \"где взять X\"","metadata":{}},{"cell_type":"code","source":"def preprocess_datasets(el):\n    phrase = el['phrase']\n    intent = el['intent']\n    \n    labels = le.transform([intent]).item()\n    ret = tokenizer(phrase)\n    ret['labels'] = labels\n    \n    return ret\n\ntokenizer = AutoTokenizer.from_pretrained('ai-forever/ruBert-base')\nle = LabelEncoder().fit(train_df['intent'].unique().tolist())\ncolumns_remove = [\"phrase\", \"intent\", \"token_type_ids\"]\ntrain_dataset = Dataset.from_dict(train_df).map(preprocess_datasets).remove_columns(columns_remove)\nval_dataset = Dataset.from_dict(valid_df).map(preprocess_datasets).remove_columns(columns_remove)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:19:30.057080Z","iopub.execute_input":"2024-04-28T11:19:30.057471Z","iopub.status.idle":"2024-04-28T11:19:38.942034Z","shell.execute_reply.started":"2024-04-28T11:19:30.057436Z","shell.execute_reply":"2024-04-28T11:19:38.941233Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/590 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f85f9d084814b87b2b50c866a6f520d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"956af5d61f8c4bd5acd1d1e4ae5577dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13230 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78cc8e1253d24399b8b03fba68e93e52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/883 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bb8a6e6420443c190756919d6c81dbc"}},"metadata":{}}]},{"cell_type":"markdown","source":"Для начала обучим обычный RuBERT","metadata":{}},{"cell_type":"code","source":"class RuBertIntentsClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained('ai-forever/ruBert-base')\n        for param in self.bert.parameters():\n            param.requires_grad = False\n        self.classifier = nn.Sequential(\n                nn.Linear(768, 768),\n                nn.ReLU(),\n                nn.Linear(768, n_classes)\n        )\n    \n    def forward(self, input_ids, attention_mask, labels):\n        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n        out = self.classifier(out.mean(dim=1))\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:19:38.943325Z","iopub.execute_input":"2024-04-28T11:19:38.943955Z","iopub.status.idle":"2024-04-28T11:19:38.951116Z","shell.execute_reply.started":"2024-04-28T11:19:38.943920Z","shell.execute_reply":"2024-04-28T11:19:38.950237Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"roflan = RuBertIntentsClassifier(len(le.classes_))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:58:25.480554Z","iopub.execute_input":"2024-04-21T15:58:25.480896Z","iopub.status.idle":"2024-04-21T15:58:26.592090Z","shell.execute_reply.started":"2024-04-21T15:58:25.480869Z","shell.execute_reply":"2024-04-21T15:58:26.591233Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def compute_loss(model, inputs, return_outputs=False):\n    labels = inputs.get(\"labels\")\n    logits = model(**inputs)\n    loss = nn.CrossEntropyLoss()(logits.squeeze(), labels.squeeze())\n    if return_outputs:\n        return loss, {\"logits\" : logits}\n    return loss\n\ndef compute_accuracy(eval_preds):\n    preds, targets = eval_preds\n    preds = torch.tensor(preds)\n    preds = preds.argmax(dim=-1)\n    targets = torch.tensor(targets)\n    \n    accuracy = (preds == targets).float()\n\n    return {'accuracy' : accuracy.mean()}","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:19:38.952285Z","iopub.execute_input":"2024-04-28T11:19:38.952606Z","iopub.status.idle":"2024-04-28T11:19:38.964725Z","shell.execute_reply.started":"2024-04-28T11:19:38.952576Z","shell.execute_reply":"2024-04-28T11:19:38.963882Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    '/kaggle/working',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit = 1,\n    learning_rate=3e-4,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=50,\n    weight_decay=0.01,\n#     lr_scheduler_type='cosine',\n    metric_for_best_model=\"accuracy\",\n    load_best_model_at_end=True,\n    remove_unused_columns=False,\n    label_names=['labels'],\n    report_to=\"wandb\"\n)\n\ntrainer = Trainer(\n        roflan,\n        training_args,                             \n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        data_collator=DataCollatorWithPadding(tokenizer),\n        compute_metrics=compute_accuracy,\n)\ntrainer.compute_loss = compute_loss","metadata":{"execution":{"iopub.status.busy":"2024-04-27T18:35:47.968662Z","iopub.execute_input":"2024-04-27T18:35:47.969057Z","iopub.status.idle":"2024-04-27T18:35:48.957623Z","shell.execute_reply.started":"2024-04-27T18:35:47.969030Z","shell.execute_reply":"2024-04-27T18:35:48.956389Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Эмбеддинги RuBERT информативные, поэтому получается обучая только классификатор добиться аккураси 94% на валидации. В то же время обучение всей модели целиком было бы затруднительно в силу небольших размеров датасета.","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:58:41.568145Z","iopub.execute_input":"2024-04-21T15:58:41.568809Z","iopub.status.idle":"2024-04-21T16:09:40.335316Z","shell.execute_reply.started":"2024-04-21T15:58:41.568775Z","shell.execute_reply":"2024-04-21T16:09:40.333238Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240421_155842-an5xbaum</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/xdoni4/huggingface/runs/an5xbaum' target=\"_blank\">lilac-firebrand-28</a></strong> to <a href='https://wandb.ai/xdoni4/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/xdoni4/huggingface' target=\"_blank\">https://wandb.ai/xdoni4/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/xdoni4/huggingface/runs/an5xbaum' target=\"_blank\">https://wandb.ai/xdoni4/huggingface/runs/an5xbaum</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20700' max='20700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20700/20700 10:25, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.252835</td>\n      <td>0.718007</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.348800</td>\n      <td>0.645431</td>\n      <td>0.839185</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.764300</td>\n      <td>0.429534</td>\n      <td>0.884485</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.443200</td>\n      <td>0.366865</td>\n      <td>0.884485</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.336000</td>\n      <td>0.314460</td>\n      <td>0.902605</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.336000</td>\n      <td>0.271009</td>\n      <td>0.906002</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.273100</td>\n      <td>0.259468</td>\n      <td>0.912797</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.238200</td>\n      <td>0.242494</td>\n      <td>0.913930</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.215900</td>\n      <td>0.230208</td>\n      <td>0.916195</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.198800</td>\n      <td>0.216452</td>\n      <td>0.919592</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.189400</td>\n      <td>0.214227</td>\n      <td>0.917327</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.189400</td>\n      <td>0.213442</td>\n      <td>0.922990</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.175300</td>\n      <td>0.219604</td>\n      <td>0.917327</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.169800</td>\n      <td>0.196645</td>\n      <td>0.922990</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.160100</td>\n      <td>0.194023</td>\n      <td>0.926387</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.159600</td>\n      <td>0.188877</td>\n      <td>0.925255</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.155200</td>\n      <td>0.180574</td>\n      <td>0.927520</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.155200</td>\n      <td>0.190778</td>\n      <td>0.932050</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.147800</td>\n      <td>0.178470</td>\n      <td>0.929785</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.146500</td>\n      <td>0.187211</td>\n      <td>0.928652</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.137900</td>\n      <td>0.192132</td>\n      <td>0.927520</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.135600</td>\n      <td>0.174487</td>\n      <td>0.932050</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.135100</td>\n      <td>0.187836</td>\n      <td>0.925255</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.135100</td>\n      <td>0.183947</td>\n      <td>0.935447</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.137700</td>\n      <td>0.179273</td>\n      <td>0.928652</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.128600</td>\n      <td>0.169101</td>\n      <td>0.934315</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.125800</td>\n      <td>0.183359</td>\n      <td>0.932050</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.129700</td>\n      <td>0.174214</td>\n      <td>0.928652</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.125500</td>\n      <td>0.175934</td>\n      <td>0.930917</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.125500</td>\n      <td>0.175680</td>\n      <td>0.929785</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.125100</td>\n      <td>0.169817</td>\n      <td>0.936580</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.119500</td>\n      <td>0.167842</td>\n      <td>0.928652</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.120500</td>\n      <td>0.167136</td>\n      <td>0.941110</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.119000</td>\n      <td>0.167465</td>\n      <td>0.936580</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.119000</td>\n      <td>0.168664</td>\n      <td>0.936580</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.120100</td>\n      <td>0.163080</td>\n      <td>0.941110</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.114500</td>\n      <td>0.167722</td>\n      <td>0.935447</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.120400</td>\n      <td>0.166494</td>\n      <td>0.937712</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.110700</td>\n      <td>0.168791</td>\n      <td>0.930917</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.117400</td>\n      <td>0.162769</td>\n      <td>0.937712</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.117400</td>\n      <td>0.164221</td>\n      <td>0.933182</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.109700</td>\n      <td>0.163570</td>\n      <td>0.943375</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.108100</td>\n      <td>0.164992</td>\n      <td>0.935447</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.111600</td>\n      <td>0.160730</td>\n      <td>0.941110</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.110900</td>\n      <td>0.161814</td>\n      <td>0.936580</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.109600</td>\n      <td>0.160618</td>\n      <td>0.939977</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.109600</td>\n      <td>0.161654</td>\n      <td>0.941110</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.107200</td>\n      <td>0.162624</td>\n      <td>0.939977</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.109600</td>\n      <td>0.161956</td>\n      <td>0.941110</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.109400</td>\n      <td>0.161387</td>\n      <td>0.939977</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=20700, training_loss=0.22130508901992282, metrics={'train_runtime': 657.9526, 'train_samples_per_second': 1005.392, 'train_steps_per_second': 31.461, 'total_flos': 0.0, 'train_loss': 0.22130508901992282, 'epoch': 50.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T16:13:29.380555Z","iopub.execute_input":"2024-04-21T16:13:29.380998Z","iopub.status.idle":"2024-04-21T16:13:29.969395Z","shell.execute_reply.started":"2024-04-21T16:13:29.380967Z","shell.execute_reply":"2024-04-21T16:13:29.968247Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [28/28 00:00]\n    </div>\n    "},"metadata":{}},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.1635701060295105,\n 'eval_accuracy': 0.9433748722076416,\n 'eval_runtime': 0.5793,\n 'eval_samples_per_second': 1524.279,\n 'eval_steps_per_second': 48.335,\n 'epoch': 50.0}"},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T16:13:35.539960Z","iopub.execute_input":"2024-04-21T16:13:35.540371Z","iopub.status.idle":"2024-04-21T16:13:39.479081Z","shell.execute_reply.started":"2024-04-21T16:13:35.540321Z","shell.execute_reply":"2024-04-21T16:13:39.478370Z"},"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.024 MB uploaded\\r'), FloatProgress(value=0.05482802145787315, max=1.…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▅▆▆▇▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>eval/loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▇█▅▃▅▄▃▂▂▄▁▃▄▂▂▃▂▃▃▃▃▃▂▂▂▃▄▂▂▃▂▂▂▂▄▃▃▃▆▇</td></tr><tr><td>eval/samples_per_second</td><td>▂▁▄▆▄▅▆▇▇▅█▆▅▇▇▆▇▆▆▆▆▆▇▇▇▆▅▇▇▆▇▇▇▇▅▅▆▆▃▂</td></tr><tr><td>eval/steps_per_second</td><td>▂▁▄▆▄▅▆▇▇▅█▆▅▇▇▆▇▆▆▆▆▆▇▇▇▆▅▇▇▆▇▇▇▇▅▅▆▆▃▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>█▆▇▆▃▂▄▃▃▅▃▂▆▄▄▂▁▃▂▄▄▂▃▄▃▂▄▄▄▃▃▃▃▃▃▄▅▂▆▃</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.94337</td></tr><tr><td>eval/loss</td><td>0.16357</td></tr><tr><td>eval/runtime</td><td>0.5793</td></tr><tr><td>eval/samples_per_second</td><td>1524.279</td></tr><tr><td>eval/steps_per_second</td><td>48.335</td></tr><tr><td>train/epoch</td><td>50.0</td></tr><tr><td>train/global_step</td><td>20700</td></tr><tr><td>train/grad_norm</td><td>1.18078</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1094</td></tr><tr><td>train/total_flos</td><td>0.0</td></tr><tr><td>train/train_loss</td><td>0.22131</td></tr><tr><td>train/train_runtime</td><td>657.9526</td></tr><tr><td>train/train_samples_per_second</td><td>1005.392</td></tr><tr><td>train/train_steps_per_second</td><td>31.461</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lilac-firebrand-28</strong> at: <a href='https://wandb.ai/xdoni4/huggingface/runs/an5xbaum' target=\"_blank\">https://wandb.ai/xdoni4/huggingface/runs/an5xbaum</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240421_155842-an5xbaum/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"Теперь реализуемый сетап, в котором можно добавлять новые классы","metadata":{}},{"cell_type":"code","source":"class RuBertIntentsClassifierWithAddition(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained('ai-forever/ruBert-base')\n        for param in self.bert.parameters():\n            param.requires_grad = False\n        self.clf_inner = nn.Sequential(\n                nn.Linear(768, 768),\n                nn.ReLU(),\n        )\n        self.classifier = nn.Sequential(\n                nn.Linear(768, n_classes)\n        )\n        self.added = nn.ParameterList([])\n    \n    def forward(self, input_ids, attention_mask, labels):\n        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n        out = self.clf_inner(out)\n        outs = []\n        outs.append(self.classifier(out.mean(dim=1)))\n        for module in self.added:\n            outs.append(module(out.mean(dim=1)))\n        \n        return torch.cat(outs, dim=-1)\n    \n    def add(self, n_classes):\n        module = nn.Linear(768, n_classes)\n        self.added.append(module)\n        \n    def freeze_except_newest(self):\n        for param in self.clf_inner.parameters():\n            param.requires_grad = False\n        \n        for param in self.classifier.parameters():\n            param.requires_grad = False\n        \n        for module in self.added:\n            if module == self.added[-1]:\n                break\n            for param in module.parameters():\n                param.requires_grad = False\n    \n    def unfreeze_all(self):\n        for param in self.clf_inner.parameters():\n            param.requires_grad = True\n        \n        for param in self.classifier.parameters():\n            param.requires_grad = True\n        \n        for module in self.added:\n            for param in module.parameters():\n                param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:25:55.967528Z","iopub.execute_input":"2024-04-27T20:25:55.968372Z","iopub.status.idle":"2024-04-27T20:25:55.980456Z","shell.execute_reply.started":"2024-04-27T20:25:55.968340Z","shell.execute_reply":"2024-04-27T20:25:55.979564Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"Обучимся на 120 классов","metadata":{}},{"cell_type":"code","source":"train_df_120 = train_df[train_df[\"intent\"].apply(lambda x : x in le.classes_[:120])]\nvalid_df_120 = valid_df[valid_df[\"intent\"].apply(lambda x : x in le.classes_[:120])]\n\ntrain_dataset_120 = Dataset.from_dict(train_df_120).map(preprocess_datasets).remove_columns(columns_remove)\nval_dataset_120 = Dataset.from_dict(valid_df_120).map(preprocess_datasets).remove_columns(columns_remove)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:22:22.147496Z","iopub.execute_input":"2024-04-27T20:22:22.148315Z","iopub.status.idle":"2024-04-27T20:22:28.328484Z","shell.execute_reply.started":"2024-04-27T20:22:22.148282Z","shell.execute_reply":"2024-04-27T20:22:28.327509Z"},"trusted":true},"execution_count":72,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/11420 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27c817b61bf1497cbd9921a854eb9812"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/765 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"753c22a2c56a4deaa0848c2901f8c90f"}},"metadata":{}}]},{"cell_type":"code","source":"roflan = RuBertIntentsClassifierWithAddition(120)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:26:05.784819Z","iopub.execute_input":"2024-04-27T20:26:05.785674Z","iopub.status.idle":"2024-04-27T20:26:06.753054Z","shell.execute_reply.started":"2024-04-27T20:26:05.785639Z","shell.execute_reply":"2024-04-27T20:26:06.752181Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    '/kaggle/working',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit = 1,\n    learning_rate=3e-4,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=50,\n    weight_decay=0.01,\n#     lr_scheduler_type='cosine',\n    metric_for_best_model=\"accuracy\",\n    load_best_model_at_end=True,\n    remove_unused_columns=False,\n    label_names=['labels'],\n    report_to=\"wandb\"\n)\n\ntrainer = Trainer(\n        roflan,\n        training_args,                             \n        train_dataset=train_dataset_120,\n        eval_dataset=val_dataset_120,\n        data_collator=DataCollatorWithPadding(tokenizer),\n        compute_metrics=compute_accuracy,\n)\ntrainer.compute_loss = compute_loss","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:26:10.348950Z","iopub.execute_input":"2024-04-27T20:26:10.349293Z","iopub.status.idle":"2024-04-27T20:26:10.558502Z","shell.execute_reply.started":"2024-04-27T20:26:10.349264Z","shell.execute_reply":"2024-04-27T20:26:10.557484Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:26:14.774462Z","iopub.execute_input":"2024-04-27T20:26:14.774847Z","iopub.status.idle":"2024-04-27T20:36:12.432861Z","shell.execute_reply.started":"2024-04-27T20:26:14.774817Z","shell.execute_reply":"2024-04-27T20:36:12.431894Z"},"trusted":true},"execution_count":84,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240427_202615-m4xylub2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/xdoni4/huggingface/runs/m4xylub2' target=\"_blank\">faithful-thunder-35</a></strong> to <a href='https://wandb.ai/xdoni4/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/xdoni4/huggingface' target=\"_blank\">https://wandb.ai/xdoni4/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/xdoni4/huggingface/runs/m4xylub2' target=\"_blank\">https://wandb.ai/xdoni4/huggingface/runs/m4xylub2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='17850' max='17850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [17850/17850 09:24, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.306631</td>\n      <td>0.690196</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.135800</td>\n      <td>0.685618</td>\n      <td>0.830065</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.725700</td>\n      <td>0.484466</td>\n      <td>0.861438</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.725700</td>\n      <td>0.383283</td>\n      <td>0.883660</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.447300</td>\n      <td>0.323829</td>\n      <td>0.888889</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.339300</td>\n      <td>0.306319</td>\n      <td>0.888889</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.339300</td>\n      <td>0.277140</td>\n      <td>0.904575</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.283300</td>\n      <td>0.251261</td>\n      <td>0.916340</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.247000</td>\n      <td>0.250489</td>\n      <td>0.912418</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.228800</td>\n      <td>0.239732</td>\n      <td>0.907190</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.228800</td>\n      <td>0.224053</td>\n      <td>0.909804</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.210000</td>\n      <td>0.218314</td>\n      <td>0.920261</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.195200</td>\n      <td>0.217310</td>\n      <td>0.926797</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.195200</td>\n      <td>0.208584</td>\n      <td>0.920261</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.194600</td>\n      <td>0.207416</td>\n      <td>0.909804</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.176900</td>\n      <td>0.213118</td>\n      <td>0.917647</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.178600</td>\n      <td>0.210496</td>\n      <td>0.911111</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.178600</td>\n      <td>0.198529</td>\n      <td>0.912418</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.166400</td>\n      <td>0.191602</td>\n      <td>0.917647</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.160800</td>\n      <td>0.190646</td>\n      <td>0.920261</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.160800</td>\n      <td>0.196846</td>\n      <td>0.922876</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.157000</td>\n      <td>0.187521</td>\n      <td>0.929412</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.155900</td>\n      <td>0.183230</td>\n      <td>0.928105</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.150800</td>\n      <td>0.182885</td>\n      <td>0.934641</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.150800</td>\n      <td>0.170776</td>\n      <td>0.928105</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.153700</td>\n      <td>0.176896</td>\n      <td>0.934641</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.144000</td>\n      <td>0.173489</td>\n      <td>0.928105</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.144000</td>\n      <td>0.173910</td>\n      <td>0.930719</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.143100</td>\n      <td>0.172244</td>\n      <td>0.934641</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.138600</td>\n      <td>0.171924</td>\n      <td>0.929412</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.142900</td>\n      <td>0.178710</td>\n      <td>0.929412</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.142900</td>\n      <td>0.178176</td>\n      <td>0.925490</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.136500</td>\n      <td>0.175308</td>\n      <td>0.928105</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.135100</td>\n      <td>0.171705</td>\n      <td>0.930719</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.135100</td>\n      <td>0.166479</td>\n      <td>0.935948</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.134500</td>\n      <td>0.167403</td>\n      <td>0.928105</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.132300</td>\n      <td>0.165295</td>\n      <td>0.934641</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.132200</td>\n      <td>0.164929</td>\n      <td>0.930719</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.132200</td>\n      <td>0.166366</td>\n      <td>0.933333</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.129000</td>\n      <td>0.170009</td>\n      <td>0.930719</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.126000</td>\n      <td>0.163973</td>\n      <td>0.933333</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.126000</td>\n      <td>0.160402</td>\n      <td>0.934641</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.127300</td>\n      <td>0.160927</td>\n      <td>0.935948</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.124800</td>\n      <td>0.161454</td>\n      <td>0.932026</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.122400</td>\n      <td>0.159157</td>\n      <td>0.930719</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.122400</td>\n      <td>0.158731</td>\n      <td>0.935948</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.123000</td>\n      <td>0.157201</td>\n      <td>0.935948</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.120000</td>\n      <td>0.158529</td>\n      <td>0.934641</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.120000</td>\n      <td>0.158426</td>\n      <td>0.933333</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.122700</td>\n      <td>0.158574</td>\n      <td>0.932026</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=17850, training_loss=0.24168076459099264, metrics={'train_runtime': 597.1113, 'train_samples_per_second': 956.271, 'train_steps_per_second': 29.894, 'total_flos': 0.0, 'train_loss': 0.24168076459099264, 'epoch': 50.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:36:12.434855Z","iopub.execute_input":"2024-04-27T20:36:12.435136Z","iopub.status.idle":"2024-04-27T20:36:12.930592Z","shell.execute_reply.started":"2024-04-27T20:36:12.435110Z","shell.execute_reply":"2024-04-27T20:36:12.929505Z"},"trusted":true},"execution_count":85,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24/24 00:00]\n    </div>\n    "},"metadata":{}},{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.1664794385433197,\n 'eval_accuracy': 0.9359477162361145,\n 'eval_runtime': 0.4859,\n 'eval_samples_per_second': 1574.269,\n 'eval_steps_per_second': 49.389,\n 'epoch': 50.0}"},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:36:12.931467Z","iopub.execute_input":"2024-04-27T20:36:12.931763Z","iopub.status.idle":"2024-04-27T20:36:58.602981Z","shell.execute_reply.started":"2024-04-27T20:36:12.931737Z","shell.execute_reply":"2024-04-27T20:36:58.601990Z"},"trusted":true},"execution_count":86,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▅▆▇▇▇▇▇▇██▇▇▇▇█████████████████████████</td></tr><tr><td>eval/loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▃▁▂▂▁▁█▂▂▂▂▂▃▃▂▂▃▄▂▆▄▂▂▂▁▃▆▁▃▄▄▆▃▂▃▃▂▃▄</td></tr><tr><td>eval/samples_per_second</td><td>▇▅█▇▇██▁▇▇▇▇▇▆▆▇▇▆▅▇▃▅▇▇▇█▆▃█▆▅▅▃▆▆▆▆▇▆▅</td></tr><tr><td>eval/steps_per_second</td><td>▇▅█▇▇██▁▇▇▇▇▇▆▆▇▇▆▅▇▃▅▇▇▇█▆▃█▆▅▅▃▆▆▆▆▇▆▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>█▇▇▆▇▅▄▅▄▅▄▅▅▃▇▄▆▅▃▄▄▆▄▆▄▁▂▃▄▄▂▅▂▃▃</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.93595</td></tr><tr><td>eval/loss</td><td>0.16648</td></tr><tr><td>eval/runtime</td><td>0.4859</td></tr><tr><td>eval/samples_per_second</td><td>1574.269</td></tr><tr><td>eval/steps_per_second</td><td>49.389</td></tr><tr><td>train/epoch</td><td>50.0</td></tr><tr><td>train/global_step</td><td>17850</td></tr><tr><td>train/grad_norm</td><td>1.27656</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.1227</td></tr><tr><td>train/total_flos</td><td>0.0</td></tr><tr><td>train/train_loss</td><td>0.24168</td></tr><tr><td>train/train_runtime</td><td>597.1113</td></tr><tr><td>train/train_samples_per_second</td><td>956.271</td></tr><tr><td>train/train_steps_per_second</td><td>29.894</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">faithful-thunder-35</strong> at: <a href='https://wandb.ai/xdoni4/huggingface/runs/m4xylub2' target=\"_blank\">https://wandb.ai/xdoni4/huggingface/runs/m4xylub2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240427_202615-m4xylub2/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"Теперь дообучимся еще на 22 класса","metadata":{}},{"cell_type":"code","source":"roflan.add(22)\nroflan.freeze_except_newest()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:36:58.604961Z","iopub.execute_input":"2024-04-27T20:36:58.605258Z","iopub.status.idle":"2024-04-27T20:36:58.610212Z","shell.execute_reply.started":"2024-04-27T20:36:58.605232Z","shell.execute_reply":"2024-04-27T20:36:58.609429Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"roflan.added.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:36:58.611201Z","iopub.execute_input":"2024-04-27T20:36:58.611473Z","iopub.status.idle":"2024-04-27T20:36:58.624434Z","shell.execute_reply.started":"2024-04-27T20:36:58.611444Z","shell.execute_reply":"2024-04-27T20:36:58.623607Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"ParameterList(\n    (0): Object of type: Linear\n  (0): Linear(in_features=768, out_features=22, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    '/kaggle/working',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit = 1,\n    learning_rate=3e-4,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=50,\n    weight_decay=0.01,\n#     lr_scheduler_type='cosine',\n    metric_for_best_model=\"accuracy\",\n    load_best_model_at_end=True,\n    remove_unused_columns=False,\n    label_names=['labels'],\n    report_to=\"wandb\"\n)\n\ntrainer = Trainer(\n        roflan,\n        training_args,                             \n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        data_collator=DataCollatorWithPadding(tokenizer),\n        compute_metrics=compute_accuracy,\n)\ntrainer.compute_loss = compute_loss","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:36:58.625537Z","iopub.execute_input":"2024-04-27T20:36:58.627195Z","iopub.status.idle":"2024-04-27T20:36:58.644252Z","shell.execute_reply.started":"2024-04-27T20:36:58.627167Z","shell.execute_reply":"2024-04-27T20:36:58.643389Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:36:58.645472Z","iopub.execute_input":"2024-04-27T20:36:58.646821Z","iopub.status.idle":"2024-04-27T20:48:00.362886Z","shell.execute_reply.started":"2024-04-27T20:36:58.646795Z","shell.execute_reply":"2024-04-27T20:48:00.361868Z"},"trusted":true},"execution_count":90,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240427_203659-lo9n4knn</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/xdoni4/huggingface/runs/lo9n4knn' target=\"_blank\">curious-dream-36</a></strong> to <a href='https://wandb.ai/xdoni4/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/xdoni4/huggingface' target=\"_blank\">https://wandb.ai/xdoni4/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/xdoni4/huggingface/runs/lo9n4knn' target=\"_blank\">https://wandb.ai/xdoni4/huggingface/runs/lo9n4knn</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20700' max='20700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20700/20700 10:29, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.332169</td>\n      <td>0.892412</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.458000</td>\n      <td>0.263332</td>\n      <td>0.913930</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.241000</td>\n      <td>0.227130</td>\n      <td>0.920725</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.189700</td>\n      <td>0.212908</td>\n      <td>0.922990</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.175000</td>\n      <td>0.201400</td>\n      <td>0.929785</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.175000</td>\n      <td>0.194955</td>\n      <td>0.933182</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.163700</td>\n      <td>0.196946</td>\n      <td>0.929785</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.156300</td>\n      <td>0.191510</td>\n      <td>0.934315</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.155700</td>\n      <td>0.184957</td>\n      <td>0.935447</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.148200</td>\n      <td>0.183529</td>\n      <td>0.933182</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.145700</td>\n      <td>0.180613</td>\n      <td>0.936580</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.145700</td>\n      <td>0.179265</td>\n      <td>0.937712</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.144100</td>\n      <td>0.178138</td>\n      <td>0.937712</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.145300</td>\n      <td>0.176414</td>\n      <td>0.936580</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.136900</td>\n      <td>0.179040</td>\n      <td>0.936580</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.143200</td>\n      <td>0.175549</td>\n      <td>0.937712</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.140600</td>\n      <td>0.177149</td>\n      <td>0.936580</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.140600</td>\n      <td>0.175394</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.138400</td>\n      <td>0.175255</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.137800</td>\n      <td>0.175994</td>\n      <td>0.936580</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.133400</td>\n      <td>0.171481</td>\n      <td>0.937712</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.131200</td>\n      <td>0.173790</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.132700</td>\n      <td>0.174821</td>\n      <td>0.936580</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.132700</td>\n      <td>0.171434</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.137800</td>\n      <td>0.173200</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.131300</td>\n      <td>0.172735</td>\n      <td>0.936580</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.130300</td>\n      <td>0.169699</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.135800</td>\n      <td>0.170371</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.133500</td>\n      <td>0.173751</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.133500</td>\n      <td>0.171113</td>\n      <td>0.937712</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.133800</td>\n      <td>0.169325</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.130100</td>\n      <td>0.168855</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.131300</td>\n      <td>0.169736</td>\n      <td>0.937712</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.133000</td>\n      <td>0.169176</td>\n      <td>0.937712</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.133000</td>\n      <td>0.170561</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.132300</td>\n      <td>0.170227</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.128000</td>\n      <td>0.168149</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.134400</td>\n      <td>0.167712</td>\n      <td>0.937712</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.126400</td>\n      <td>0.169621</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.133400</td>\n      <td>0.169553</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.133400</td>\n      <td>0.170593</td>\n      <td>0.937712</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.126500</td>\n      <td>0.168437</td>\n      <td>0.937712</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.124900</td>\n      <td>0.170455</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.130300</td>\n      <td>0.168964</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.130800</td>\n      <td>0.168262</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.129400</td>\n      <td>0.167962</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.129400</td>\n      <td>0.168337</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.127900</td>\n      <td>0.168835</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.130000</td>\n      <td>0.168650</td>\n      <td>0.938845</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.131200</td>\n      <td>0.168570</td>\n      <td>0.938845</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=20700, training_loss=0.14851047727796768, metrics={'train_runtime': 661.1483, 'train_samples_per_second': 1000.532, 'train_steps_per_second': 31.309, 'total_flos': 0.0, 'train_loss': 0.14851047727796768, 'epoch': 50.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:52:25.173338Z","iopub.execute_input":"2024-04-27T20:52:25.174564Z","iopub.status.idle":"2024-04-27T20:52:25.763207Z","shell.execute_reply.started":"2024-04-27T20:52:25.174515Z","shell.execute_reply":"2024-04-27T20:52:25.762135Z"},"trusted":true},"execution_count":91,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [28/28 00:00]\n    </div>\n    "},"metadata":{}},{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.17539402842521667,\n 'eval_accuracy': 0.9388448596000671,\n 'eval_runtime': 0.5785,\n 'eval_samples_per_second': 1526.422,\n 'eval_steps_per_second': 48.403,\n 'epoch': 50.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"Результат получился похуже, чем если обучать сразу на все классы.","metadata":{}},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:25:22.261249Z","iopub.execute_input":"2024-04-27T20:25:22.261975Z","iopub.status.idle":"2024-04-27T20:25:25.379420Z","shell.execute_reply.started":"2024-04-27T20:25:22.261939Z","shell.execute_reply":"2024-04-27T20:25:25.378444Z"},"trusted":true},"execution_count":80,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.054 MB of 0.054 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▅█</td></tr><tr><td>eval/loss</td><td>█▄▁</td></tr><tr><td>eval/runtime</td><td>█▁▄</td></tr><tr><td>eval/samples_per_second</td><td>▁█▅</td></tr><tr><td>eval/steps_per_second</td><td>▁█▅</td></tr><tr><td>train/epoch</td><td>▁▂▅▆█</td></tr><tr><td>train/global_step</td><td>▁▂▄▆█</td></tr><tr><td>train/grad_norm</td><td>▁█</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.08607</td></tr><tr><td>eval/loss</td><td>4.70992</td></tr><tr><td>eval/runtime</td><td>0.5644</td></tr><tr><td>eval/samples_per_second</td><td>1564.52</td></tr><tr><td>eval/steps_per_second</td><td>49.611</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>1242</td></tr><tr><td>train/grad_norm</td><td>0.48066</td></tr><tr><td>train/learning_rate</td><td>0.00029</td></tr><tr><td>train/loss</td><td>4.7919</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">classic-frost-34</strong> at: <a href='https://wandb.ai/xdoni4/huggingface/runs/dixes11q' target=\"_blank\">https://wandb.ai/xdoni4/huggingface/runs/dixes11q</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240427_202407-dixes11q/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"Реализуем аугментации. Поскольку тексты очень короткие, и учитывая специфику данных, наиболее типичными ошибками будут орфографические и опечатки, поэтому их и будем использовать для аугментации","metadata":{}},{"cell_type":"code","source":"char_aug = CharAug(\n    unit_prob=0, # Percentage of the phrase to which augmentations will be applied\n    min_aug=1, # Minimum number of augmentations\n    max_aug=2, # Maximum number of augmentations\n    lang=\"rus\", # supports: \"rus\", \"eng\"\n    platform=\"pc\", # supports: \"pc\", \"mobile\"\n    random_seed=228,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:19:38.967783Z","iopub.execute_input":"2024-04-28T11:19:38.968073Z","iopub.status.idle":"2024-04-28T11:19:38.976211Z","shell.execute_reply.started":"2024-04-28T11:19:38.968050Z","shell.execute_reply":"2024-04-28T11:19:38.975387Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на пример аугментации","metadata":{}},{"cell_type":"code","source":"text = \"привет дядя анзон\"\nchar_aug.augment(char_aug.augment(text=text, action=\"orfo\"), action=\"typo\")","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:20:10.317807Z","iopub.execute_input":"2024-04-28T11:20:10.318638Z","iopub.status.idle":"2024-04-28T11:20:10.327323Z","shell.execute_reply.started":"2024-04-28T11:20:10.318589Z","shell.execute_reply":"2024-04-28T11:20:10.326382Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'привет бядя анзон'"},"metadata":{}}]},{"cell_type":"code","source":"def augment_wrapper(x):\n    phrase = x[\"phrase\"]\n    intent = x[\"intent\"][0]\n    phrase = char_aug.augment(char_aug.augment(text=phrase, action=\"orfo\"), action=\"typo\")\n    boba = preprocess_datasets({\"phrase\" : phrase, \"intent\" : intent})\n    boba.pop('token_type_ids')\n    return pd.DataFrame.from_dict(pd.DataFrame.from_dict({k : [v] for k, v in boba.items()}))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:20:17.905545Z","iopub.execute_input":"2024-04-28T11:20:17.906189Z","iopub.status.idle":"2024-04-28T11:20:17.912533Z","shell.execute_reply.started":"2024-04-28T11:20:17.906154Z","shell.execute_reply":"2024-04-28T11:20:17.911532Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_ds = Dataset.from_dict(train_df)\ntrain_ds.set_transform(lambda x : augment_wrapper(x))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:20:23.038686Z","iopub.execute_input":"2024-04-28T11:20:23.039433Z","iopub.status.idle":"2024-04-28T11:20:23.054142Z","shell.execute_reply.started":"2024-04-28T11:20:23.039400Z","shell.execute_reply":"2024-04-28T11:20:23.053262Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_ds[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:20:24.157801Z","iopub.execute_input":"2024-04-28T11:20:24.158659Z","iopub.status.idle":"2024-04-28T11:20:24.166679Z","shell.execute_reply.started":"2024-04-28T11:20:24.158624Z","shell.execute_reply":"2024-04-28T11:20:24.165671Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 1098, 182, 389, 10067, 66508, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1],\n 'labels': 114}"},"metadata":{}}]},{"cell_type":"code","source":"train_ds[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:20:26.127912Z","iopub.execute_input":"2024-04-28T11:20:26.128852Z","iopub.status.idle":"2024-04-28T11:20:26.136415Z","shell.execute_reply.started":"2024-04-28T11:20:26.128821Z","shell.execute_reply":"2024-04-28T11:20:26.135515Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 1098, 982, 10067, 66508, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1],\n 'labels': 114}"},"metadata":{}}]},{"cell_type":"code","source":"roflan = RuBertIntentsClassifier(len(le.classes_))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:20:30.267372Z","iopub.execute_input":"2024-04-28T11:20:30.267746Z","iopub.status.idle":"2024-04-28T11:20:34.591233Z","shell.execute_reply.started":"2024-04-28T11:20:30.267717Z","shell.execute_reply":"2024-04-28T11:20:34.590197Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/716M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44a37d5f9d0d42d08d1bd3bf0500828a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    '/kaggle/working',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit = 1,\n    learning_rate=3e-4,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=50,\n    weight_decay=0.01,\n#     lr_scheduler_type='cosine',\n    metric_for_best_model=\"accuracy\",\n    load_best_model_at_end=True,\n    remove_unused_columns=False,\n    label_names=['labels'],\n    report_to=\"wandb\"\n)\n\ntrainer = Trainer(\n        roflan,\n        training_args,                             \n        train_dataset=train_ds,\n        eval_dataset=val_dataset,\n        data_collator=DataCollatorWithPadding(tokenizer),\n        compute_metrics=compute_accuracy,\n)\ntrainer.compute_loss = compute_loss","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:20:48.311694Z","iopub.execute_input":"2024-04-28T11:20:48.312576Z","iopub.status.idle":"2024-04-28T11:20:48.691586Z","shell.execute_reply.started":"2024-04-28T11:20:48.312542Z","shell.execute_reply":"2024-04-28T11:20:48.690777Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:20:52.477650Z","iopub.execute_input":"2024-04-28T11:20:52.478382Z","iopub.status.idle":"2024-04-28T11:43:31.241386Z","shell.execute_reply.started":"2024-04-28T11:20:52.478350Z","shell.execute_reply":"2024-04-28T11:43:31.240319Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240428_112104-whiapv7f</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/xdoni4/huggingface/runs/whiapv7f' target=\"_blank\">polar-forest-38</a></strong> to <a href='https://wandb.ai/xdoni4/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/xdoni4/huggingface' target=\"_blank\">https://wandb.ai/xdoni4/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/xdoni4/huggingface/runs/whiapv7f' target=\"_blank\">https://wandb.ai/xdoni4/huggingface/runs/whiapv7f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20700' max='20700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20700/20700 21:52, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.732982</td>\n      <td>0.605889</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.982700</td>\n      <td>1.015515</td>\n      <td>0.746319</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.520300</td>\n      <td>0.688396</td>\n      <td>0.806342</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.060900</td>\n      <td>0.560012</td>\n      <td>0.841450</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.849700</td>\n      <td>0.455593</td>\n      <td>0.866365</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.849700</td>\n      <td>0.394860</td>\n      <td>0.879955</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.733200</td>\n      <td>0.363581</td>\n      <td>0.881087</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.652100</td>\n      <td>0.319162</td>\n      <td>0.895810</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.601200</td>\n      <td>0.326110</td>\n      <td>0.892412</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.557900</td>\n      <td>0.290869</td>\n      <td>0.899207</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.536500</td>\n      <td>0.280115</td>\n      <td>0.904870</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.536500</td>\n      <td>0.269330</td>\n      <td>0.901472</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.500400</td>\n      <td>0.265251</td>\n      <td>0.903737</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.472700</td>\n      <td>0.247398</td>\n      <td>0.917327</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.456900</td>\n      <td>0.239455</td>\n      <td>0.915062</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.457300</td>\n      <td>0.242098</td>\n      <td>0.908267</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.439500</td>\n      <td>0.238177</td>\n      <td>0.915062</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.439500</td>\n      <td>0.229694</td>\n      <td>0.917327</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.425700</td>\n      <td>0.230397</td>\n      <td>0.919592</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.418700</td>\n      <td>0.242824</td>\n      <td>0.911665</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.404900</td>\n      <td>0.219938</td>\n      <td>0.925255</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.390300</td>\n      <td>0.219659</td>\n      <td>0.919592</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.392200</td>\n      <td>0.220004</td>\n      <td>0.918460</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.392200</td>\n      <td>0.222022</td>\n      <td>0.922990</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.370900</td>\n      <td>0.213412</td>\n      <td>0.920725</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.372000</td>\n      <td>0.207799</td>\n      <td>0.916195</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.362000</td>\n      <td>0.209524</td>\n      <td>0.925255</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.366800</td>\n      <td>0.201197</td>\n      <td>0.926387</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.357200</td>\n      <td>0.205225</td>\n      <td>0.919592</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.357200</td>\n      <td>0.205503</td>\n      <td>0.927520</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.355600</td>\n      <td>0.199577</td>\n      <td>0.924122</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.337100</td>\n      <td>0.205793</td>\n      <td>0.921857</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.336200</td>\n      <td>0.203059</td>\n      <td>0.921857</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.338600</td>\n      <td>0.198734</td>\n      <td>0.926387</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.338600</td>\n      <td>0.203366</td>\n      <td>0.921857</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.340800</td>\n      <td>0.194215</td>\n      <td>0.933182</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.326100</td>\n      <td>0.196845</td>\n      <td>0.921857</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.323200</td>\n      <td>0.191477</td>\n      <td>0.925255</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.324700</td>\n      <td>0.196233</td>\n      <td>0.920725</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.329200</td>\n      <td>0.194094</td>\n      <td>0.924122</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.329200</td>\n      <td>0.191430</td>\n      <td>0.925255</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.326000</td>\n      <td>0.191824</td>\n      <td>0.927520</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.308300</td>\n      <td>0.190647</td>\n      <td>0.926387</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.313800</td>\n      <td>0.187520</td>\n      <td>0.932050</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.314900</td>\n      <td>0.186579</td>\n      <td>0.926387</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.316500</td>\n      <td>0.184232</td>\n      <td>0.924122</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.316500</td>\n      <td>0.187297</td>\n      <td>0.929785</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.310600</td>\n      <td>0.187078</td>\n      <td>0.928652</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.305000</td>\n      <td>0.184255</td>\n      <td>0.930917</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.308300</td>\n      <td>0.184733</td>\n      <td>0.929785</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=20700, training_loss=0.5148038348138044, metrics={'train_runtime': 1358.2803, 'train_samples_per_second': 487.013, 'train_steps_per_second': 15.24, 'total_flos': 0.0, 'train_loss': 0.5148038348138044, 'epoch': 50.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:43:31.831894Z","iopub.execute_input":"2024-04-28T11:43:31.832560Z","iopub.status.idle":"2024-04-28T11:43:32.443904Z","shell.execute_reply.started":"2024-04-28T11:43:31.832533Z","shell.execute_reply":"2024-04-28T11:43:32.442745Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.19421540200710297,\n 'eval_accuracy': 0.9331823587417603,\n 'eval_runtime': 0.5986,\n 'eval_samples_per_second': 1475.029,\n 'eval_steps_per_second': 46.773,\n 'epoch': 50.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"Реузультат получился чуть хуже чем для остальных моделей. На этом датасете не было проблем с переобучением и без аугментаций","metadata":{}},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:44:26.303454Z","iopub.execute_input":"2024-04-28T11:44:26.304114Z","iopub.status.idle":"2024-04-28T11:44:29.865972Z","shell.execute_reply.started":"2024-04-28T11:44:26.304083Z","shell.execute_reply":"2024-04-28T11:44:29.865244Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.024 MB uploaded\\r'), FloatProgress(value=0.05475402081362346, max=1.…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▅▆▇▇▇▇▇▇██▇███████████████████████████</td></tr><tr><td>eval/loss</td><td>█▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▁▄▂▂▂▁▂▃▄▃▃▂▃▁▃▄▂▂▃▂▆▃▄▂▃▂▃▂▂▃▃█▄▂▄▅▂▃▇</td></tr><tr><td>eval/samples_per_second</td><td>▅█▅▇▇▇█▇▆▅▆▆▇▆█▆▅▆▇▆▇▃▆▅▇▆▇▅▇▇▆▆▁▅▇▅▄▇▆▂</td></tr><tr><td>eval/steps_per_second</td><td>▅█▅▇▇▇█▇▆▅▆▆▇▆█▆▅▆▇▆▇▃▆▅▇▆▇▅▇▇▆▆▁▅▇▅▄▇▆▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▇▆▇▇▄▄▃▅▃▄▄█▄▅▄▂▃▃▃▄▄▂▄▄▄▄▆▄▅▅▂▃▄▄▃▅▅▄▆▁</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.93318</td></tr><tr><td>eval/loss</td><td>0.19422</td></tr><tr><td>eval/runtime</td><td>0.5986</td></tr><tr><td>eval/samples_per_second</td><td>1475.029</td></tr><tr><td>eval/steps_per_second</td><td>46.773</td></tr><tr><td>train/epoch</td><td>50.0</td></tr><tr><td>train/global_step</td><td>20700</td></tr><tr><td>train/grad_norm</td><td>1.27828</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3083</td></tr><tr><td>train/total_flos</td><td>0.0</td></tr><tr><td>train/train_loss</td><td>0.5148</td></tr><tr><td>train/train_runtime</td><td>1358.2803</td></tr><tr><td>train/train_samples_per_second</td><td>487.013</td></tr><tr><td>train/train_steps_per_second</td><td>15.24</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">polar-forest-38</strong> at: <a href='https://wandb.ai/xdoni4/huggingface/runs/whiapv7f' target=\"_blank\">https://wandb.ai/xdoni4/huggingface/runs/whiapv7f</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240428_112104-whiapv7f/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"С точки зрения метрики лучшей получилась модель, которая обучается на все классы сразу, потом та, где дообучались, потом модель с аугментациями. Преимущества дообучения в том, что можно распространить модель на новые классы без переобучения уже обученных весов для остальных классов (это может быть существенно, если на обучение требуется много времени), и вообще позволяет хорошо масштабировать модель. По времени обучения получается сопоставимо с моделью, но чуть быстрее за счет того, что меньше классов. Последняя модель самая долгая, потому что там нельзя сгенерить датасет сразу же, нужно делать случаные аугментации на каждой эпохе. Модель становится более робастной, но в данном случае не удается хорошо это проследить, потому что при обучении предыдущих моделей лоссы на трейне и на валидации все время уменьшаются, не возникает переобучения. Вообще, можно было бы попробовать получить семантические аугментации, потому что самое важное  при обучениие трансформеров: это большая выборка, а имеющиеся у нас выборки не такие большие. Можно попробовать использовать идею из LLaVA с генерацией по описанию с помощью LLM.","metadata":{}}]}